{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries\n",
    "We rely heavily on `numpy` and `pandas` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data set\n",
    "We select the data from the data directory. The `parse_dates` option directly converts the `datetime` column to the right `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "df = pd.read_csv(os.path.join(data_dir, 'data.csv'), parse_dates=['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split\n",
    "The data span two years of activity. In a realistic setting, it is most probable that data from past years would be used to predict on current and future years. We thus simulate such a production run by choosing the first year for training, and second year for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = df['datetime'] < '2012-01-01'\n",
    "test_index = df['datetime'] >= '2012-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We delay the split into two data frames to the very last section, for keeping feature engineering simple. In a production run though, any feature engineering process should be factorized into the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correcting `dtypes`\n",
    "Some `dtypes` are wrong, e.g. the `humidity` column is represented as `int64` instead of `float64`. Since we plan to normalize continuous features, we should better correct this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['humidity'] = df['humidity'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, categorical columns should fit in the `category dtypes`. The `pd.Categorical` method allows to do this, and can even be provided with a list of allowed categories, preventing discrepancies between categorical features in the training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_categories = {'holiday': [0, 1], 'workingday': [0, 1], 'season': [1, 2, 3, 4], 'weather': [1, 2, 3, 4]}\n",
    "for column, categories in allowed_categories.items():\n",
    "    df[column] = pd.Categorical(df[column], categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummifying categorical variables\n",
    "The `pd.get_dummies` function creates one column per category, filled with boolean 0s and 1s. These features are 100% correlated, and this would impair linear regression (non-invertible matrix). We thus remove the column corresponding to the first category, which is useless anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "where_category = df.dtypes == 'category'\n",
    "categorical_features = df.dtypes[where_category].index.tolist()\n",
    "for column in categorical_features:\n",
    "    categories = df[column].cat.categories\n",
    "    df = pd.get_dummies(df, columns=[column]).drop(columns=[column + '_' + str(min(categories))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing outliers\n",
    "Inspecting the `weather` column, it turns out that the event `weather == 4` is almost non-existant, except for one observation. We thus have no valuable information on this kind of weather. Would it occur in the future, we should better assign it a similar weather for which we have more information, such as `weather == 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "where_weather_4 = df['weather_4'] == 1\n",
    "df.loc[where_weather_4, 'weather_3'] = 1\n",
    "df.drop(columns=['weather_4'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing datetime\n",
    "The `datetime` column is internally represented as a series of increasing large `int`s. We can make it more meaningful to the model by extracting the hour, which influences strongly the `count` variable. Furthermore, there is a periodicity in the signal (see descriptive statistics). Appealing to Fourrier series, we can guess that sinusoidal functions of the time variable may contribute significantly to the explanation of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega_t = 2*np.pi*df['datetime'].dt.hour/24\n",
    "for i in range(1, 7):\n",
    "    df['hour_cos' + str(i)] = np.cos(i*omega_t)\n",
    "    df['hour_sin' + str(i)] = np.sin(i*omega_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated features\n",
    "The idea is to use past targets to predict the current one. In a production scenario, it would be perfectly legal to use the `count`s of January to predict those of February. Past counts encodes the recent number of locations, thus anticipating increase or decrease of locations in time. Since data is missing after the $20^{th}$ of each month, we cannot rely on sliding windows of size less than a month. We can still take advantage of the `resample` method: we create a feature `moving_avg` that is nothing but the average number of daily locations in the past month. The `shift` method ensures that any observation at month $n$ is provided with the average of month $n-1$. Obviously, the first month is subject to missing data, that we fill with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = df[['datetime', 'count']].resample('1M', on='datetime').sum().shift(1).fillna(0)/19\n",
    "moving_avg.columns = ['count_1M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `moving_avg` data frame has one line per month, and we should input these values in the whole data frame `df`. We first convert the index of `moving_avg` to a string of the form YYYYMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = moving_avg.index.to_series()\n",
    "moving_avg.set_index(index.dt.year.map(str) + index.dt.month.map(str), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then repeat the operation for `df`, creating a `df_yyyymm` index, on the base of which `moving_avg` computations are duplicated as need be to fit `df` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yyyymm = df['datetime'].dt.year.map(str) + df['datetime'].dt.month.map(str)\n",
    "moving_avg = moving_avg.loc[df_yyyymm].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new column `moving_avg` is simply obtained by `concat`enation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, moving_avg], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "The `atemp` variable is almost 100% correlated with the `temp` one, so we can safely drop it from the features, all the more since it would bring high variance for linear regression. We have no more use of `datetime` and `count` columns either, the latter being trivially deduced from the two outputs `casual` and `registered`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['datetime', 'atemp', 'count'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a reasonable number of features, with no so much correlations, and we thus do not need *a priori* further dimension reduction techniques such as Principal Component Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-transformation\n",
    "The count variable being widely spread, with most of values near zero, we apply a log-transformation of the `casual` and `registered` variables, to shorten their range of variations. This is also going to favor the modelisation of multiplicative effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['casual', 'registered']\n",
    "for target in target_columns:\n",
    "    df[target] = np.log(1 + df[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archiving\n",
    "Sometimes, feature engineering is so demanding of computational resources that it is performed offline, and stored in backup files. We also perform the train/test split once and for all by creating multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[train_index]\n",
    "df_test = df[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrange the project by creating a `pickles` directory in which we are going to save binary files. The `pathlib` library allows to create the directory if not existing, thus automatizing the whole process. We are also going to write into separate files the features and the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'pickles'\n",
    "pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using binary files (pickles) is that they keep in memory all metadata, such as `dtypes`, whereas `csv` files can only store strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(columns=target_columns).to_pickle(os.path.join(save_dir, 'train.pkl'))\n",
    "df_test.drop(columns=target_columns).to_pickle(os.path.join(save_dir, 'test.pkl'))\n",
    "df_train[target_columns].to_pickle(os.path.join(save_dir, 'y_train.pkl'))\n",
    "df_test[target_columns].to_pickle(os.path.join(save_dir, 'y_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
